{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai langchain langchain-openai faiss-cpu python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19529884",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carrega as variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configura a chave de API (opcional, a biblioteca openai já faz isso se a variável de ambiente estiver definida)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b025a07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Inicializa o modelo de linguagem\n",
    "# O gpt-3.5-turbo é uma ótima opção por ser rápido e barato\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# Inicializa a memória para guardar o histórico da conversa\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Cria a \"Chain\" de conversação, unindo o LLM e a memória\n",
    "conversation_chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True # verbose=True nos ajuda a ver o que está acontecendo por trás dos panos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdd0f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Primeira interação: O assistente se apresenta e começa a qualificação\n",
    "prompt_inicial = \"Olá! Sou o assistente virtual da Welhome. Para começarmos, pode me dizer quantos imóveis você possui para alugar e onde eles se localizam?\"\n",
    "resposta_assistente = conversation_chain.predict(input=prompt_inicial)\n",
    "print(resposta_assistente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ef3af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Segunda interação: O usuário responde, e o assistente continua a qualificação [cite: 53]\n",
    "prompt_usuario_1 = \"Tenho 2 apartamentos em São Paulo, na Vila Madalena.\"\n",
    "resposta_assistente_2 = conversation_chain.predict(input=prompt_usuario_1)\n",
    "print(resposta_assistente_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c3d2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Terceira interação: O usuário responde à última pergunta [cite: 53]\n",
    "prompt_usuario_2 = \"Minha experiência é baixa, sempre aluguei por conta própria e tive alguns problemas.\"\n",
    "resposta_assistente_3 = conversation_chain.predict(input=prompt_usuario_2)\n",
    "print(resposta_assistente_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b698c75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Pega todo o histórico da conversa que está na memória\n",
    "historico_conversa = memory.buffer_as_str\n",
    "\n",
    "# Cria um template de prompt para a tarefa de sumarização\n",
    "prompt_template_resumo = PromptTemplate(\n",
    "    input_variables=[\"historico\"],\n",
    "    template=\"\"\"\n",
    "    Com base no histórico de conversa abaixo, gere um resumo estruturado para um vendedor humano.\n",
    "    O resumo deve ser claro, conciso e conter as informações mais importantes do lead.\n",
    "    Use o seguinte formato:\n",
    "    - **Quantidade de Imóveis:** [Número de imóveis]\n",
    "    - **Localização:** [Cidade e bairro]\n",
    "    - **Experiência Prévia:** [Descrição da experiência]\n",
    "    - **Principal Dor/Necessidade:** [Inferir a principal necessidade do lead a partir da conversa]\n",
    "\n",
    "    Histórico da Conversa:\n",
    "    {historico}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Cria uma nova \"Chain\" apenas para a tarefa de resumo\n",
    "summarization_chain = prompt_template_resumo | llm\n",
    "\n",
    "# Executa a chain de resumo, passando o histórico\n",
    "resumo_para_vendedor = summarization_chain.invoke({\"historico\": historico_conversa})\n",
    "\n",
    "print(\"--- RESUMO PARA O VENDEDOR ---\")\n",
    "print(resumo_para_vendedor.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b48c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "faq_welhome = \"\"\"\n",
    "Pergunta: Como a Welhome garante que o aluguel será pago em dia?\n",
    "Resposta: A Welhome oferece a garantia de aluguel em dia. Mesmo que o inquilino atrase, nós garantimos o repasse do valor para o proprietário na data combinada, sem custos adicionais.\n",
    "\n",
    "Pergunta: Quem é responsável pela manutenção do imóvel?\n",
    "Resposta: Reparos estruturais são de responsabilidade do proprietário. Pequenas manutenções do dia a dia, como troca de lâmpadas, são do inquilino. Nossa plataforma ajuda a intermediar e orçar serviços de manutenção quando necessário.\n",
    "\n",
    "Pergunta: Qual é o custo da taxa de administração da Welhome?\n",
    "Resposta: Nossa taxa de administração é de 8% sobre o valor do aluguel. Essa taxa cobre a gestão completa do seu imóvel, incluindo divulgação, gestão de contratos, repasse garantido e suporte.\n",
    "\n",
    "Pergunta: Como funciona a vistoria do imóvel?\n",
    "Resposta: Realizamos uma vistoria profissional completa, com fotos e vídeos, antes da entrada do inquilino e após a sua saída. Isso garante que o imóvel seja devolvido nas mesmas condições em que foi entregue.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f04c16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# 1. Dividir (Chunk): Quebra o texto do FAQ em pedaços menores\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.create_documents([faq_welhome])\n",
    "\n",
    "# 2. Vetorizar e Armazenar (Embed & Store): Cria os embeddings e armazena no FAISS [cite: 59]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 3. Criar um \"Retriever\": um objeto que sabe como buscar informações no vector_store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# 4. Criar o Prompt para o RAG: Instruí o LLM a usar o contexto recuperado\n",
    "prompt_rag_template = PromptTemplate.from_template(\"\"\"\n",
    "Responda a pergunta do usuário com base apenas no contexto fornecido.\n",
    "Se a resposta não estiver no contexto, diga \"Não encontrei essa informação na minha base de dados.\".\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# 5. Criar a \"Chain\" de RAG: Junta o prompt, o LLM e o retriever\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt_rag_template)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"Setup do RAG concluído. Pronto para receber perguntas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e30e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pergunta cuja resposta ESTÁ no FAQ\n",
    "pergunta_1 = \"O que acontece se o meu inquilino não pagar o aluguel?\"\n",
    "resposta_1 = rag_chain.invoke({\"input\": pergunta_1})\n",
    "\n",
    "print(f\"Pergunta: {pergunta_1}\")\n",
    "print(f\"Resposta: {resposta_1['answer']}\\n\")\n",
    "\n",
    "\n",
    "# Pergunta cuja resposta também ESTÁ no FAQ\n",
    "pergunta_2 = \"Quanto custa o serviço de vocês?\"\n",
    "resposta_2 = rag_chain.invoke({\"input\": pergunta_2})\n",
    "\n",
    "print(f\"Pergunta: {pergunta_2}\")\n",
    "print(f\"Resposta: {resposta_2['answer']}\\n\")\n",
    "\n",
    "\n",
    "# Pergunta cuja resposta NÃO ESTÁ no FAQ\n",
    "pergunta_3 = \"Vocês administram imóveis comerciais?\"\n",
    "resposta_3 = rag_chain.invoke({\"input\": pergunta_3})\n",
    "\n",
    "print(f\"Pergunta: {pergunta_3}\")\n",
    "print(f\"Resposta: {resposta_3['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
